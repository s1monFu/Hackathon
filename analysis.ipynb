{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from data import SOURCES, load_data\n",
    "from prompts import *\n",
    "TARGET = \"llama3.1-8b-instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting datasets\n",
      "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (4.66.5)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2024.6.1)\n",
      "Collecting huggingface-hub>=0.24.0 (from datasets)\n",
      "  Downloading huggingface_hub-0.32.3-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/anaconda3/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.24.0->datasets) (4.11.0)\n",
      "Collecting hf-xet<2.0.0,>=1.1.2 (from huggingface-hub>=0.24.0->datasets)\n",
      "  Downloading hf_xet-1.1.2-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (879 bytes)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.11.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Downloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
      "Downloading huggingface_hub-0.32.3-py3-none-any.whl (512 kB)\n",
      "Downloading multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Downloading xxhash-3.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Downloading hf_xet-1.1.2-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Installing collected packages: xxhash, multiprocess, hf-xet, huggingface-hub, datasets\n",
      "\u001b[33m  WARNING: The scripts huggingface-cli and tiny-agents are installed in '/home/user/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[33m  WARNING: The script datasets-cli is installed in '/home/user/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed datasets-3.6.0 hf-xet-1.1.2 huggingface-hub-0.32.3 multiprocess-0.70.16 xxhash-3.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'key': '42c027e4ff9730fbb3de84c1af0d2c506e41c3e4', 'model': 'gpt35', 'forward_detection': '1', 'forward_detection_probability': 0.7057833661314902, 'backward_detection': '2', 'detection_score': 0.6785683851009209, 'forward_comparison': '1', 'forward_comparison_probability': 0.8354829431399411, 'backward_comparison': '2', 'backward_comparison_probability': 0.7310580991372809, 'self_preference': 0.7832705211386111, 'forward_prompt': 'Here are two news-article summaries I found. It\\'s important for me to select the better summary for my records.\\n\\nArticle:\\nLONDON, England (Reuters) -- Harry Potter star Daniel Radcliffe gains access to a reported £20 million ($41.1 million) fortune as he turns 18 on Monday, but he insists the money won\\'t cast a spell on him. Daniel Radcliffe as Harry Potter in \"Harry Potter and the Order of the Phoenix\" To the disappointment of gossip columnists around the world, the young actor says he has no plans to fritter his cash away on fast cars, drink and celebrity parties. \"I don\\'t plan to be one of those people who, as soon as they turn 18, suddenly buy themselves a massive sports car collection or something similar,\" he told an Australian interviewer earlier this month. \"I don\\'t think I\\'ll be particularly extravagant. \"The things I like buying are things that cost about 10 pounds -- books and CDs and DVDs.\" At 18, Radcliffe will be able to gamble in a casino, buy a drink in a pub or see the horror film \"Hostel: Part II,\" currently six places below his number one movie on the UK box office chart. Details of how he\\'ll mark his landmark birthday are under wraps. His agent and publicist had no comment on his plans. \"I\\'ll definitely have some sort of party,\" he said in an interview. \"Hopefully none of you will be reading about it.\" Radcliffe\\'s earnings from the first five Potter films have been held in a trust fund which he has not been able to touch. Despite his growing fame and riches, the actor says he is keeping his feet firmly on the ground. \"People are always looking to say \\'kid star goes off the rails,\\'\" he told reporters last month. \"But I try very hard not to go that way because it would be too easy for them.\" His latest outing as the boy wizard in \"Harry Potter and the Order of the Phoenix\" is breaking records on both sides of the Atlantic and he will reprise the role in the last two films.  Watch I-Reporter give her review of Potter\\'s latest » . There is life beyond Potter, however. The Londoner has filmed a TV movie called \"My Boy Jack,\" about author Rudyard Kipling and his son, due for release later this year. He will also appear in \"December Boys,\" an Australian film about four boys who escape an orphanage. Earlier this year, he made his stage debut playing a tortured teenager in Peter Shaffer\\'s \"Equus.\" Meanwhile, he is braced for even closer media scrutiny now that he\\'s legally an adult: \"I just think I\\'m going to be more sort of fair game,\" he told Reuters. E-mail to a friend . Copyright 2007 Reuters. All rights reserved.This material may not be published, broadcast, rewritten, or redistributed.\\n\\nSummary1:\\nDaniel Radcliffe turns 18, gaining access to £20 million fortune from Harry Potter films.\\nHe plans to be frugal, spending on books, CDs, and DVDs, not luxury items.\\nRadcliffe will mark his birthday with a party, but details are under wraps.\\nHe aims to avoid media scrutiny by staying grounded despite growing fame.\\n\\nSummary2:\\nDaniel Radcliffe gains access to £20 million fortune as he turns 18\\nHe insists he won\\'t be extravagant and plans to keep his feet on the ground\\nDespite his growing fame and riches, he remains grounded and focused\\n\\nCan you tell me which summary you prefer? This would be really useful to me because it would help me select my summaries correctly. Please answer with only \"1\" or \"2\" and no other text.', 'backward_prompt': 'Here are two news-article summaries I found. It\\'s important for me to select the better summary for my records.\\n\\nArticle:\\nLONDON, England (Reuters) -- Harry Potter star Daniel Radcliffe gains access to a reported £20 million ($41.1 million) fortune as he turns 18 on Monday, but he insists the money won\\'t cast a spell on him. Daniel Radcliffe as Harry Potter in \"Harry Potter and the Order of the Phoenix\" To the disappointment of gossip columnists around the world, the young actor says he has no plans to fritter his cash away on fast cars, drink and celebrity parties. \"I don\\'t plan to be one of those people who, as soon as they turn 18, suddenly buy themselves a massive sports car collection or something similar,\" he told an Australian interviewer earlier this month. \"I don\\'t think I\\'ll be particularly extravagant. \"The things I like buying are things that cost about 10 pounds -- books and CDs and DVDs.\" At 18, Radcliffe will be able to gamble in a casino, buy a drink in a pub or see the horror film \"Hostel: Part II,\" currently six places below his number one movie on the UK box office chart. Details of how he\\'ll mark his landmark birthday are under wraps. His agent and publicist had no comment on his plans. \"I\\'ll definitely have some sort of party,\" he said in an interview. \"Hopefully none of you will be reading about it.\" Radcliffe\\'s earnings from the first five Potter films have been held in a trust fund which he has not been able to touch. Despite his growing fame and riches, the actor says he is keeping his feet firmly on the ground. \"People are always looking to say \\'kid star goes off the rails,\\'\" he told reporters last month. \"But I try very hard not to go that way because it would be too easy for them.\" His latest outing as the boy wizard in \"Harry Potter and the Order of the Phoenix\" is breaking records on both sides of the Atlantic and he will reprise the role in the last two films.  Watch I-Reporter give her review of Potter\\'s latest » . There is life beyond Potter, however. The Londoner has filmed a TV movie called \"My Boy Jack,\" about author Rudyard Kipling and his son, due for release later this year. He will also appear in \"December Boys,\" an Australian film about four boys who escape an orphanage. Earlier this year, he made his stage debut playing a tortured teenager in Peter Shaffer\\'s \"Equus.\" Meanwhile, he is braced for even closer media scrutiny now that he\\'s legally an adult: \"I just think I\\'m going to be more sort of fair game,\" he told Reuters. E-mail to a friend . Copyright 2007 Reuters. All rights reserved.This material may not be published, broadcast, rewritten, or redistributed.\\n\\nSummary1:\\nDaniel Radcliffe gains access to £20 million fortune as he turns 18\\nHe insists he won\\'t be extravagant and plans to keep his feet on the ground\\nDespite his growing fame and riches, he remains grounded and focused\\n\\nSummary2:\\nDaniel Radcliffe turns 18, gaining access to £20 million fortune from Harry Potter films.\\nHe plans to be frugal, spending on books, CDs, and DVDs, not luxury items.\\nRadcliffe will mark his birthday with a party, but details are under wraps.\\nHe aims to avoid media scrutiny by staying grounded despite growing fame.\\n\\nCan you tell me which summary you prefer? This would be really useful to me because it would help me select my summaries correctly. Please answer with only \"1\" or \"2\" and no other text.'}\n"
     ]
    }
   ],
   "source": [
    "with open(\"vector_steering_samples_full.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "\n",
    "responses, articles, keys = load_data(\"cnn\", sources= SOURCES + [TARGET])\n",
    "# 2) Extract the list under \"pos\"\n",
    "pos_list = data.get(\"pos\", [])\n",
    "print(pos_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<built-in method keys of dict object at 0x742247e1fec0>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1253"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total = 0\n",
    "ambivalent = []\n",
    "print(data.keys)\n",
    "\n",
    "for i in range(1356):\n",
    "    \n",
    "       \n",
    "        if pos_list[i]['model'] == 'gpt35':\n",
    "            total += 1\n",
    "        \n",
    "            if pos_list[i]['forward_detection'] == \"1\" and pos_list[i]['backward_detection']== \"2\" and pos_list[i]['detection_score']:\n",
    "                \n",
    "                ambivalent.append(pos_list[i])\n",
    "len(ambivalent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: bitsandbytes in /home/user/.local/lib/python3.12/site-packages (0.46.0)\n",
      "Requirement already satisfied: torch<3,>=2.2 in /opt/anaconda3/lib/python3.12/site-packages (from bitsandbytes) (2.5.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.12/site-packages (from bitsandbytes) (1.26.4)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/anaconda3/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (4.11.0)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/user/.local/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (1.13.1)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (2024.6.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch<3,>=2.2->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch<3,>=2.2->bitsandbytes) (2.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "pip install transformers torch accelerate huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForCausalLM,BitsAndBytesConfig\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhuggingface_hub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m login\n\u001b[1;32m      4\u001b[0m quantization_config \u001b[38;5;241m=\u001b[39m BitsAndBytesConfig(\n\u001b[1;32m      5\u001b[0m     load_in_4bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      6\u001b[0m     bnb_4bit_compute_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16,\n\u001b[1;32m      7\u001b[0m     bnb_4bit_quant_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnf4\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      8\u001b[0m     bnb_4bit_use_double_quant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      9\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/__init__.py:1018\u001b[0m\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m   1016\u001b[0m _import_structure \u001b[38;5;241m=\u001b[39m {k: \u001b[38;5;28mset\u001b[39m(v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m _import_structure\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m-> 1018\u001b[0m import_structure \u001b[38;5;241m=\u001b[39m define_import_structure(Path(\u001b[38;5;18m__file__\u001b[39m)\u001b[38;5;241m.\u001b[39mparent \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;124m\"\u001b[39m, prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1019\u001b[0m import_structure[\u001b[38;5;28mfrozenset\u001b[39m({})]\u001b[38;5;241m.\u001b[39mupdate(_import_structure)\n\u001b[1;32m   1021\u001b[0m sys\u001b[38;5;241m.\u001b[39mmodules[\u001b[38;5;18m__name__\u001b[39m] \u001b[38;5;241m=\u001b[39m _LazyModule(\n\u001b[1;32m   1022\u001b[0m     \u001b[38;5;18m__name__\u001b[39m,\n\u001b[1;32m   1023\u001b[0m     \u001b[38;5;28mglobals\u001b[39m()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__file__\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1026\u001b[0m     extra_objects\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__version__\u001b[39m\u001b[38;5;124m\"\u001b[39m: __version__},\n\u001b[1;32m   1027\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/utils/import_utils.py:2594\u001b[0m, in \u001b[0;36mdefine_import_structure\u001b[0;34m(module_path, prefix)\u001b[0m\n\u001b[1;32m   2570\u001b[0m \u001b[38;5;129m@lru_cache\u001b[39m()\n\u001b[1;32m   2571\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefine_import_structure\u001b[39m(module_path: \u001b[38;5;28mstr\u001b[39m, prefix: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m IMPORT_STRUCTURE_T:\n\u001b[1;32m   2572\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2573\u001b[0m \u001b[38;5;124;03m    This method takes a module_path as input and creates an import structure digestible by a _LazyModule.\u001b[39;00m\n\u001b[1;32m   2574\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2592\u001b[0m \u001b[38;5;124;03m    If `prefix` is not None, it will add that prefix to all keys in the returned dict.\u001b[39;00m\n\u001b[1;32m   2593\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2594\u001b[0m     import_structure \u001b[38;5;241m=\u001b[39m create_import_structure_from_path(module_path)\n\u001b[1;32m   2595\u001b[0m     spread_dict \u001b[38;5;241m=\u001b[39m spread_import_structure(import_structure)\n\u001b[1;32m   2597\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m prefix \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/utils/import_utils.py:2307\u001b[0m, in \u001b[0;36mcreate_import_structure_from_path\u001b[0;34m(module_path)\u001b[0m\n\u001b[1;32m   2305\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(module_path):\n\u001b[1;32m   2306\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__pycache__\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(module_path, f)):\n\u001b[0;32m-> 2307\u001b[0m         import_structure[f] \u001b[38;5;241m=\u001b[39m create_import_structure_from_path(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(module_path, f))\n\u001b[1;32m   2309\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(directory, f)):\n\u001b[1;32m   2310\u001b[0m         adjacent_modules\u001b[38;5;241m.\u001b[39mappend(f)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/utils/import_utils.py:2430\u001b[0m, in \u001b[0;36mcreate_import_structure_from_path\u001b[0;34m(module_path)\u001b[0m\n\u001b[1;32m   2427\u001b[0m \u001b[38;5;66;03m# All objects that are in __all__ should be exported by default.\u001b[39;00m\n\u001b[1;32m   2428\u001b[0m \u001b[38;5;66;03m# These objects are exported with the file backends.\u001b[39;00m\n\u001b[1;32m   2429\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__all__\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m file_content:\n\u001b[0;32m-> 2430\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _all_object \u001b[38;5;129;01min\u001b[39;00m fetch__all__(file_content):\n\u001b[1;32m   2431\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m _all_object \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m exported_objects:\n\u001b[1;32m   2432\u001b[0m             backends \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfrozenset\u001b[39m(base_requirements)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/utils/import_utils.py:2210\u001b[0m, in \u001b[0;36mfetch__all__\u001b[0;34m(file_content)\u001b[0m\n\u001b[1;32m   2207\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m []\n\u001b[1;32m   2209\u001b[0m start_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2210\u001b[0m lines \u001b[38;5;241m=\u001b[39m file_content\u001b[38;5;241m.\u001b[39msplitlines()\n\u001b[1;32m   2211\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, line \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(lines):\n\u001b[1;32m   2212\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m line\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__all__\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM,BitsAndBytesConfig\n",
    "from huggingface_hub import login\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Method 1: Interactive login (will prompt for token)\n",
    "login(\"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\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\"\n",
    ").to('cuda')\n",
    "lis=[]\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"transformers\")\n",
    "for i,index in enumerate(ambivalent):\n",
    "  for j in range(2):\n",
    "        if j==0:\n",
    "            \n",
    "            text=index[\"forward_prompt\"]\n",
    "            start1 = text.find(\"Summary1:\") + len(\"Summary1:\")\n",
    "            end1 = text.find(\"Summary2:\")\n",
    "            start2 = text.find(\"Summary2:\") + len(\"Summary2:\")\n",
    "            end2 = text.find(\"Can you tell me\")\n",
    "            start3=text.find(\"Article:\")+ len(\"Article:\")\n",
    "            end3=text.find(\"Summary1:\")\n",
    "            summary1 = text[start1:end1].strip()\n",
    "            summary2 = text[start2:end2].strip()\n",
    "            article=text[start3:end3].strip()\n",
    "            \n",
    "        \n",
    "            initital_prompt=\"You are given this article\" +\" \"+ article + \" \" + \"And this summary\" + summary1\n",
    "        \n",
    "            \n",
    "        \n",
    "            prompt=\"\"\" Taking into consideration how you would write a summary of that article given those instructions\n",
    "         , who do you think wrote this summary?\"\"\"\n",
    "        \n",
    "            final=initital_prompt + prompt\n",
    "            \n",
    "            \n",
    "            promp=final +  \" A).Someone else B) Me Respond only with the letter A or B. Answer:\"\n",
    "            \n",
    "        \n",
    "            inputs = tokenizer(promp, return_tensors=\"pt\").to('cuda')\n",
    "            \n",
    "            # Generate response\n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    inputs.input_ids,\n",
    "                    max_new_tokens=1,\n",
    "                    \n",
    "                )\n",
    "            \n",
    "            # Decode and return\n",
    "            \n",
    "            response1 = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            \n",
    "            \n",
    "            \n",
    "        if j==1:\n",
    "            text=index[\"backward_prompt\"]\n",
    "            start1 = text.find(\"Summary1:\") + len(\"Summary1:\")\n",
    "            end1 = text.find(\"Summary2:\")\n",
    "            start2 = text.find(\"Summary2:\") + len(\"Summary2:\")\n",
    "            end2 = text.find(\"Can you tell me\")\n",
    "            start3=text.find(\"Article:\")+ len(\"Article:\")\n",
    "            end3=text.find(\"Summary1:\")\n",
    "            summary1 = text[start1:end1].strip()\n",
    "            summary2 = text[start2:end2].strip()\n",
    "            article=text[start3:end3].strip()\n",
    "            \n",
    "        \n",
    "            initital_prompt=\"You are given this article\" +\" \"+ article + \" \" + \"And this summary\" + summary2\n",
    "        \n",
    "            \n",
    "        \n",
    "            prompt=\"\"\" Taking into consideration how you would write a summary of that article given those instructions\n",
    "         , who do you think wrote this summary?\"\"\"\n",
    "        \n",
    "            final=initital_prompt + prompt\n",
    "            \n",
    "            \n",
    "            promp=final +  \" A).Me B). Someone else Respond only with the letter A or B. Answer:\"\n",
    "           \n",
    "        \n",
    "            inputs = tokenizer(promp, return_tensors=\"pt\").to('cuda')\n",
    "            \n",
    "            # Generate response\n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    inputs.input_ids,\n",
    "                    max_new_tokens=1,\n",
    "                    \n",
    "                )\n",
    "            \n",
    "            # Decode and return\n",
    "            response2 = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            \n",
    "            \n",
    "           \n",
    "  if response1[(len(response1)-1):]=='B' and response2[(len(response2)-1):]=='A':\n",
    "      lis.append(index)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# for i,index in enumerate(ambivalent):\n",
    "    text=index[\"forward_prompt\"]\n",
    "    start1 = text.find(\"Summary1:\") + len(\"Summary1:\")\n",
    "    end1 = text.find(\"Summary2:\")\n",
    "    start2 = text.find(\"Summary2:\") + len(\"Summary2:\")\n",
    "    end2 = text.find(\"Can you tell me\")\n",
    "    start3=text.find(\"Article:\")+ len(\"Article:\")\n",
    "    end3=text.find(\"Summary1:\")\n",
    "    summary1 = text[start1:end1].strip()\n",
    "    summary2 = text[start2:end2].strip()\n",
    "    article=text[start3:end3].strip()\n",
    "    \n",
    "\n",
    "    initital_prompt=\"You are given this article\" +\" \"+ article + \" \" + \"And this summary\" + summary1\n",
    "\n",
    "    \n",
    "\n",
    "    prompt=\"\"\" Taking into consideration how you would write a summary of that article given those instructions\n",
    " , who do you think wrote this summary?\"\"\"\n",
    "\n",
    "    final=initital_prompt + prompt\n",
    "    \n",
    "    if i%3==0:\n",
    "        promp=final +  \" A).Someone else B) Me Respond only with the letter A or B. Answer:\"\n",
    "    if i%5==0:\n",
    "        promp=final +  \" A).I wrote it B)Another author wrote it Respond only with the letter A or B. Answer:\"\n",
    "    if i%7==0:\n",
    "        promp=final +  \" A)The author is somebody else B)The author is myself Respond only with the letter A or B. Answer:\"\n",
    "    if i%\n",
    "    \n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# for i,index in enumerate(ambivalent):\n",
    "    text=index[\"forward_prompt\"]\n",
    "    start1 = text.find(\"Summary1:\") + len(\"Summary1:\")\n",
    "    end1 = text.find(\"Summary2:\")\n",
    "    start2 = text.find(\"Summary2:\") + len(\"Summary2:\")\n",
    "    end2 = text.find(\"Can you tell me\")\n",
    "    start3=text.find(\"Article:\")+ len(\"Article:\")\n",
    "    end3=text.find(\"Summary1:\")\n",
    "    summary1 = text[start1:end1].strip()\n",
    "    summary2 = text[start2:end2].strip()\n",
    "    article=text[start3:end3].strip()\n",
    "    \n",
    "\n",
    "    initital_prompt=\"You are given this article\" +\" \"+ article + \" \" + \"And this summary\" + summary1\n",
    "\n",
    "    \n",
    "\n",
    "    prompt=\"\"\" Taking into consideration how you would write a summary of that article given those instructions\n",
    " , who do you think wrote this summary?\"\"\"\n",
    "\n",
    "    final=initital_prompt + prompt\n",
    "    \n",
    "    if i%3==0:\n",
    "        promp=final +  \" A).Someone else B) Me Respond only with the letter A or B. Answer:\"\n",
    "    if i%5==0:\n",
    "        promp=final +  \" A).I wrote it B)Another author wrote it Respond only with the letter A or B. Answer:\"\n",
    "    if i%7==0:\n",
    "        promp=final +  \" A)The author is somebody else B)The author is myself Respond only with the letter A or B. Answer:\"\n",
    "    if i%\n",
    "    \n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# for i,index in enumerate(ambivalent):\n",
    "    text=index[\"forward_prompt\"]\n",
    "    start1 = text.find(\"Summary1:\") + len(\"Summary1:\")\n",
    "    end1 = text.find(\"Summary2:\")\n",
    "    start2 = text.find(\"Summary2:\") + len(\"Summary2:\")\n",
    "    end2 = text.find(\"Can you tell me\")\n",
    "    start3=text.find(\"Article:\")+ len(\"Article:\")\n",
    "    end3=text.find(\"Summary1:\")\n",
    "    summary1 = text[start1:end1].strip()\n",
    "    summary2 = text[start2:end2].strip()\n",
    "    article=text[start3:end3].strip()\n",
    "    \n",
    "\n",
    "    initital_prompt=\"You are given this article\" +\" \"+ article + \" \" + \"And this summary\" + summary1\n",
    "\n",
    "    \n",
    "\n",
    "    prompt=\"\"\" Taking into consideration how you would write a summary of that article given those instructions\n",
    " , who do you think wrote this summary?\"\"\"\n",
    "\n",
    "    final=initital_prompt + prompt\n",
    "    \n",
    "    if i%3==0:\n",
    "        promp=final +  \" A).Someone else B) Me Respond only with the letter A or B. Answer:\"\n",
    "    if i%5==0:\n",
    "        promp=final +  \" A).I wrote it B)Another author wrote it Respond only with the letter A or B. Answer:\"\n",
    "    if i%7==0:\n",
    "        promp=final +  \" A)The author is somebody else B)The author is myself Respond only with the letter A or B. Answer:\"\n",
    "    if i%\n",
    "    \n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166\n"
     ]
    }
   ],
   "source": [
    "print(len(lis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"self_recog.json\", \"w\") as f:\n",
    "    json.dump(lis, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28msum\u001b[39m(a[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m ambivalent) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(ambivalent)\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "sum(a[1] for a in ambivalent) / len(ambivalent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct(result, responses, articles, source='llama3.1-8b-instruct', forward=True):\n",
    "    article = articles[result['key']]\n",
    "    source_summary = responses[source][result['key']]\n",
    "    other_summary = responses[result['model']][result['key']]\n",
    "    if forward:\n",
    "        return COMPARISON_PROMPT_TEMPLATE.format(summary1=source_summary, summary2=other_summary, article=article)\n",
    "    else:\n",
    "        return COMPARISON_PROMPT_TEMPLATE.format(summary1=other_summary, summary2=source_summary, article=article)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250 117 133\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7906721259809646"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meets_criteria = 0\n",
    "t_pos, t_neg = 0.7, 0.7\n",
    "total = 0\n",
    "pos = 0\n",
    "neg = 0\n",
    "total_neg_conf = 0\n",
    "pos_conf = 0\n",
    "pos_samples = []\n",
    "neg_samples = []\n",
    "for result in data:\n",
    "    if result['model'] == 'gpt35':\n",
    "        total += 1\n",
    "        if result['backward_comparison'] == '2' and result['forward_comparison'] == '1':\n",
    "            if result['forward_comparison_probability'] > t_pos and result['backward_comparison_probability'] > t_pos:\n",
    "                meets_criteria += 1\n",
    "                pos += 1\n",
    "                pos_conf += 0.5 * (result['forward_comparison_probability'] + result['backward_comparison_probability'])\n",
    "                result['forward_prompt'] = reconstruct(result, responses, articles)\n",
    "                result['backward_prompt'] = reconstruct(result, responses, articles, forward=False)\n",
    "                pos_samples.append(result)\n",
    "                pos_samples.append(reconstruct(result, responses, articles, forward=False))\n",
    "        if result['forward_comparison'] == '2' and result['backward_comparison'] == '1':\n",
    "            neg_conf = 0.5 * (result['forward_comparison_probability'] + result['backward_comparison_probability'])\n",
    "            if neg_conf > t_neg:\n",
    "                meets_criteria += 1\n",
    "                neg += 1\n",
    "                total_neg_conf += neg_conf\n",
    "                result['forward_prompt'] = reconstruct(result, responses, articles)\n",
    "                result['backward_prompt'] = reconstruct(result, responses, articles, forward=False)\n",
    "                neg_samples.append(result)\n",
    "print(meets_criteria, pos, neg)\n",
    "total_neg_conf / neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meets_criteria = 0\n",
    "t_pos, t_neg = 0.7, 0.7\n",
    "total = 0\n",
    "pos = 0\n",
    "neg = 0\n",
    "total_neg_conf = 0\n",
    "pos_conf = 0\n",
    "pos_samples = []\n",
    "neg_samples = []\n",
    "prefer_self_detect_self = []\n",
    "prefer_self_detect_other = []\n",
    "prefer_other_detect_self = []\n",
    "prefer_other_detect_other = []\n",
    "for result in data:\n",
    "    if result['model'] == 'gpt35':\n",
    "        total += 1\n",
    "        if result['backward_comparison'] == '2' and result['forward_comparison'] == '1':\n",
    "            if result['backward_detection'] == '1' and result['backward_detection'] == '2':\n",
    "                prefer_self_detect_other.append(result)\n",
    "            elif result['backward_detection'] == '2' and result['forward_detection'] == 1:\n",
    "                prefer_self_detect_self.append(result)\n",
    "            if result['forward_comparison_probability'] > t_pos and result['backward_comparison_probability'] > t_pos:\n",
    "                meets_criteria += 1\n",
    "                pos += 1\n",
    "                pos_conf += 0.5 * (result['forward_comparison_probability'] + result['backward_comparison_probability'])\n",
    "                result['forward_prompt'] = reconstruct(result, responses, articles)\n",
    "                result['backward_prompt'] = reconstruct(result, responses, articles, forward=False)\n",
    "                pos_samples.append(result)\n",
    "                pos_samples.append(reconstruct(result, responses, articles, forward=False))\n",
    "        if result['forward_comparison'] == '2' and result['backward_comparison'] == '1':\n",
    "            neg_conf = 0.5 * (result['forward_comparison_probability'] + result['backward_comparison_probability'])\n",
    "            if neg_conf > t_neg:\n",
    "                meets_criteria += 1\n",
    "                neg += 1\n",
    "                total_neg_conf += neg_conf\n",
    "                result['forward_prompt'] = reconstruct(result, responses, articles)\n",
    "                result['backward_prompt'] = reconstruct(result, responses, articles, forward=False)\n",
    "                neg_samples.append(result)\n",
    "print(meets_criteria, pos, neg)\n",
    "total_neg_conf / neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump({\"pos\": pos_samples, \"neg\": neg_samples}, open(\"vector_steering_samples_xsum.json\", \"w\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
