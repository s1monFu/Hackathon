{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Explanation of Setup\n",
        "\n",
        "These next two blocks together ensure that all necessary packages are installed, environment variables are loaded, and the LLaMA-3.1-8B model (along with its tokenizer) is initialized in an 8-bit quantized form. The first block installs `bitsandbytes` and `python-dotenv`, then imports the core libraries, `dotenv` to read our `HF_TOKEN` from a `.env` file, PyTorch for tensor operations, `tqdm` for progress bars, and the Hugging Face utilities for loading a quantized model.  \n",
        "\n",
        "The second block reads `HF_TOKEN` from the environment, configures 8-bit quantization via `BitsAndBytesConfig`, and actually downloads and prepares the LLaMA-3.1-8B tokenizer and model. Finally, it sets up padding tokens if needed, switches the model into evaluation mode, and records the number of layers and hidden-state dimension so that later code can build and manipulate steering-vector accumulators.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dCsDypY6HZ8e",
        "outputId": "dc8066fe-08a6-438a-eddd-8090dab6056f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m74.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m85.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m104.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip -q install -U bitsandbytes python-dotenv\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "import json\n",
        "import pickle\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "0bc61ab4f952436a939dd88d6d807d67",
            "1e6789972dfc4a7c86c534c798af2c53",
            "f65593664a2045368b4222a9c7a6179c",
            "90bba208638d49dbb333f25974e90ee8",
            "c9f50348c6f94090bed18d2ed47fbd8e",
            "7be4cccb457b4a1ab23153dd76f16ee8",
            "a2db30416a3a4ccea1c80434922d50ed",
            "bc3395916738413b85bb8b2c33f848a2",
            "de13cd11430b488bbd1507957d87a0b0",
            "038ad0026e8c424e8ca29f384d5b770b",
            "37d1462b47534c969f730bd3c4c1e884"
          ]
        },
        "id": "0-YL_yw2UQmy",
        "outputId": "4c593e8f-2f68-4885-8701-b68df5904693"
      },
      "outputs": [],
      "source": [
        "load_dotenv()\n",
        "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
        "quant_cfg = BitsAndBytesConfig(load_in_8bit=True)\n",
        "MODEL_ID = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
        "\n",
        "tok   = AutoTokenizer.from_pretrained(MODEL_ID, token=HF_TOKEN)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "            MODEL_ID,\n",
        "            device_map=\"auto\",\n",
        "            quantization_config=quant_cfg,\n",
        "            token=HF_TOKEN\n",
        "        )\n",
        "\n",
        "if tok.pad_token is None:\n",
        "    tok.pad_token = tok.eos_token\n",
        "    model.resize_token_embeddings(len(tok))\n",
        "model.config.pad_token_id = tok.pad_token_id\n",
        "model.eval()\n",
        "\n",
        "num_layers = model.config.num_hidden_layers\n",
        "hidden_size = model.config.hidden_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Explanation of Data Loading and Prompt Buckets\n",
        "\n",
        "In the next three blocks, we load the data and create the general data for the vectors we will create later on. We also init basic variables we will use for understandability. In this section, we first open and parse a JSON file (`vector_steering_samples_full_balanced.json`) that contains two lists of records under the keys `\"pos\"` (positive examples) and `\"neg\"` (negative examples). For each record, we extract the `\"forward_prompt\"` and `\"backward_prompt\"` strings into four separate Python lists:\n",
        "\n",
        "- `positive_forward` holds all forward‐direction prompts from the positive examples.\n",
        "- `positive_backward` holds all backward‐direction prompts from the positive examples.\n",
        "- `negative_forward` holds all forward‐direction prompts from the negative examples.\n",
        "- `negative_backward` holds all backward‐direction prompts from the negative examples.\n",
        "\n",
        "Printing the lengths of these lists confirms that we have loaded the correct number of prompts for each category.\n",
        "\n",
        "Next, we define three small sets of paired strings (`yes_no_pairs`, `self_pairs`, and `bias_pairs`) that represent “nuisance” prompt pairs. Each tuple in these lists has a “positive” version and a “negative” counterpart (e.g., `(\"Say Yes\", \"Say No\")` or `(\"Say I\", \"Say Someone\")`). We then concatenate those three lists into a single `nuisance_pairs` list.\n",
        "\n",
        "From `nuisance_pairs`, we unzip into two new lists:\n",
        "\n",
        "- `positive_nuisance_prompts` contains the first element of every pair.\n",
        "- `negative_nuisance_prompts` contains the second element of every pair.\n",
        "\n",
        "Printing their lengths provides a sanity check that we have created the correct number of nuisance prompts. These four buckets (`positive_forward`, `positive_backward`, `negative_forward`, `negative_backward`) and the two nuisance lists (`positive_nuisance_prompts`, `negative_nuisance_prompts`) will be used later to accumulate hidden‐state activations for computing the mean‐difference vectors and the nuisance vectors, respectively.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2k83xWjXz3bB",
        "outputId": "26b0fb86-b964-43cb-e5bd-51c7d0cf9bad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Positive forward prompts: 155\n",
            "Positive backward prompts: 155\n",
            "Negative forward prompts: 155\n",
            "Negative backward prompts: 155\n"
          ]
        }
      ],
      "source": [
        "with open('datasets/vector_steering_samples_full_balanced.json') as f: data = json.load(f)\n",
        "positive_forward   = [r['forward_prompt']  for r in data['pos']]\n",
        "positive_backward  = [r['backward_prompt'] for r in data['pos']]\n",
        "negative_forward   = [r['forward_prompt']  for r in data['neg']]\n",
        "negative_backward  = [r['backward_prompt'] for r in data['neg']]\n",
        "\n",
        "print(f'Positive forward prompts: {len(positive_forward)}')\n",
        "print(f'Positive backward prompts: {len(positive_backward)}')\n",
        "print(f'Negative forward prompts: {len(negative_forward)}')\n",
        "print(f'Negative backward prompts: {len(negative_backward)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7syhHl6bUQmz",
        "outputId": "99c81798-55a3-43e9-a066-79af51e6b5bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Positive nuisance prompts: 19\n",
            "Negative nuisance prompts: 19\n"
          ]
        }
      ],
      "source": [
        "# ─── DEFINE PAIRS CONCISELY AS LIST LITERALS ─────────────────────────────────────\n",
        "yes_no_pairs = [\n",
        "    (\"Say Yes\",           \"Say No\"),\n",
        "    (\"Say 1\",             \"Say 2\"),\n",
        "    (\"Say first option\",  \"Say second option\"),\n",
        "    (\"Say A\",             \"Say B\"),\n",
        "    (\"Say Agree\",         \"Say Disagree\"),\n",
        "]\n",
        "\n",
        "self_pairs = [\n",
        "    (\"Say I\",      \"Say Someone\"),\n",
        "    (\"Say I\",      \"Say He\"),\n",
        "    (\"Say I\",      \"Say She\"),\n",
        "    (\"Say Me\",     \"Say Him\"),\n",
        "    (\"Say Me\",     \"Say Her\"),\n",
        "    (\"Say My\",     \"Say His\"),\n",
        "    (\"Say My\",     \"Say Her\"),\n",
        "    (\"Say Myself\", \"Say Himself\"),\n",
        "    (\"Say Myself\", \"Say Herself\"),\n",
        "    (\"Say Mine\",   \"Say His\"),\n",
        "    (\"Say Mine\",   \"Say Hers\"),\n",
        "]\n",
        "\n",
        "bias_pairs = [\n",
        "    (\"Say Better\", \"Say Worse\"),\n",
        "    (\"Say Clear\",  \"Say Confusing\"),\n",
        "    (\"Say Helpful\",\"Say Useless\"),\n",
        "]\n",
        "\n",
        "nuisance_pairs = yes_no_pairs + self_pairs + bias_pairs\n",
        "\n",
        "positive_nuisance_prompts = [positive for positive, _ in nuisance_pairs]\n",
        "negative_nuisance_prompts = [negative for _, negative in nuisance_pairs]\n",
        "\n",
        "print(f\"Positive nuisance prompts: {len(positive_nuisance_prompts)}\")\n",
        "print(f\"Negative nuisance prompts: {len(negative_nuisance_prompts)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "HXSiU8QBUQm0"
      },
      "outputs": [],
      "source": [
        "# MEAN DIFF VECTOR VARIABLES\n",
        "num_positive = len(positive_forward) + len(positive_backward)\n",
        "num_negative = len(negative_forward) + len(negative_backward)\n",
        "\n",
        "positive_sums_by_layer = {\n",
        "    layer: [torch.zeros(hidden_size) for _ in range(10)]\n",
        "    for layer in range(num_layers)\n",
        "}\n",
        "negative_sums_by_layer = {\n",
        "    layer: [torch.zeros(hidden_size) for _ in range(10)]\n",
        "    for layer in range(num_layers)\n",
        "}\n",
        "\n",
        "# NUISANCE VECTOR VARIABLES\n",
        "positive_nuisance_prompts = [pos for pos, _ in nuisance_pairs]\n",
        "negative_nuisance_prompts = [neg for _, neg in nuisance_pairs]\n",
        "num_nuisance_pairs = len(nuisance_pairs)\n",
        "\n",
        "nuisance_positive_sums = {\n",
        "    layer: [torch.zeros(hidden_size)]\n",
        "    for layer in range(num_layers)\n",
        "}\n",
        "nuisance_negative_sums = {\n",
        "    layer: [torch.zeros(hidden_size)]\n",
        "    for layer in range(num_layers)\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Explanation of `accumulate_activations` Function\n",
        "\n",
        "This function takes a list of text prompts and, for each prompt, runs it through the LLaMA model to extract the hidden‐state activations for the last few tokens in every layer. Specifically:\n",
        "1. We iterate over each prompt (with a progress bar to show how many prompts have been processed).\n",
        "2. We tokenize the prompt once (`tok(prompt, add_special_tokens=True)`) to find out how many tokens it contains. This lets us decide how many of the final token positions (`max_tokens`) we should process.\n",
        "3. Inside a `torch.no_grad()` block (because we only need inference), we run the same prompt through the model to obtain `hidden_states` from every layer.\n",
        "4. We then loop over the last `tokens_to_process` positions (up to `max_tokens`). For each position and for each layer, we grab that layer’s hidden‐state vector for the corresponding token, move it to CPU, and add it into our pre‐allocated `sum_accumulators[layer][offset]` tensor.\n",
        "\n",
        "By the end, `sum_accumulators` holds—for each layer and each of the last `max_tokens` token positions—the sum of all hidden‐state vectors seen so far. Those sums will later be divided by the number of prompts to compute mean activations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "_CyZgZ6vUQm0"
      },
      "outputs": [],
      "source": [
        "def accumulate_activations(prompts, sum_accumulators, num_layers, max_tokens):\n",
        "    for prompt in tqdm(prompts, desc=\"Accumulating activations\"):\n",
        "        token_ids = tok(prompt, add_special_tokens=True)[\"input_ids\"]\n",
        "        tokens_to_process = min(max_tokens, len(token_ids))\n",
        "        with torch.no_grad():\n",
        "            outputs = model(\n",
        "                **tok(prompt, return_tensors=\"pt\").to(model.device),\n",
        "                output_hidden_states=True\n",
        "            )\n",
        "            hidden_states = outputs.hidden_states\n",
        "        for offset in range(tokens_to_process):\n",
        "            for layer_idx in range(num_layers):\n",
        "                vec = hidden_states[layer_idx + 1][0, -(offset + 1), :].cpu()\n",
        "                sum_accumulators[layer_idx][offset] += vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Conceptual Overview of Mean‐Difference and Nuisance Vectors\n",
        "\n",
        "In the next three blocks, we create our steering vector.\n",
        "\n",
        "At a high level, our goal is to identify directions in the transformer’s hidden‐state space that distinguish “positive” examples from “negative” examples, while also removing any unwanted or generic variation (the “nuisance” components). We do this in two stages:\n",
        "\n",
        "1. **Mean‐Difference Vectors**  \n",
        "   For each layer of the LLaMA model and for each of the last few token positions, we collect hidden‐state activations from two sets of prompts (positive vs. negative). By averaging all positive activations and subtracting the averaged negative activations, we obtain a “mean‐difference” vector that points in the direction most characteristic of positive examples at that layer and position. Intuitively, this captures the semantic or stylistic features that consistently separate positive samples from negative ones.\n",
        "\n",
        "2. **Nuisance Vectors**  \n",
        "   Many variations in hidden activations are not relevant to our positive/negative distinction—they may reflect token‐position biases, generic stylistic choices, or other confounds. To isolate those, we define small pairs of prompts that are semantically neutral (e.g., “Say Yes” vs. “Say No”, “Say I” vs. “Say Someone”), and compute their layer‐wise hidden‐state differences at the final token. By averaging across these nuisance pairs, we obtain one “nuisance” vector per layer that represents language, or position‐specific variation we want to remove.\n",
        "\n",
        "Finally, by projecting each mean‐difference vector onto the subspace orthogonal to its corresponding nuisance vector, we remove these generic confounds. The result is a set of steering vectors, one for each layer and token offset, that focus purely on the positive/negative distinction, free from nuisance variation. This two‐stage approach (mean‐difference first, nuisance subtraction second) ensures that our final steering directions capture only the task‐relevant signal.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NoM4IQp_UQm1",
        "outputId": "d0b1a8a4-b97e-4628-bbb0-cfe8828bda17"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Accumulating activations: 100%|██████████| 155/155 [00:40<00:00,  3.80it/s]\n",
            "Accumulating activations: 100%|██████████| 155/155 [00:39<00:00,  3.89it/s]\n",
            "Accumulating activations: 100%|██████████| 155/155 [00:40<00:00,  3.81it/s]\n",
            "Accumulating activations: 100%|██████████| 155/155 [00:40<00:00,  3.82it/s]\n"
          ]
        }
      ],
      "source": [
        "# ─── COMPUTE LAYER‐MEAN‐DIFFERENCE VECTORS ─────────────────────────────────────────\n",
        "# Accumulate positive vs. negative hidden states for up to last 10 token positions\n",
        "accumulate_activations(positive_forward,   positive_sums_by_layer, num_layers, 10)\n",
        "accumulate_activations(positive_backward,  positive_sums_by_layer, num_layers, 10)\n",
        "accumulate_activations(negative_forward,   negative_sums_by_layer, num_layers, 10)\n",
        "accumulate_activations(negative_backward,  negative_sums_by_layer, num_layers, 10)\n",
        "\n",
        "layer_mean_diff_vectors = defaultdict(list)\n",
        "for layer_idx in range(num_layers):\n",
        "    for offset in range(10):\n",
        "        avg_pos = positive_sums_by_layer[layer_idx][offset] / num_positive\n",
        "        avg_neg = negative_sums_by_layer[layer_idx][offset] / num_negative\n",
        "        diff    = avg_pos - avg_neg\n",
        "        normalized = diff / diff.norm()\n",
        "        #layer_mean_diff_vectors[layer_idx].append(normalized)\n",
        "        layer_mean_diff_vectors[layer_idx].append(diff)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PvTaCMj1UQm1",
        "outputId": "603dd29f-f659-47c9-ca64-a2e8375b22f4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Accumulating activations: 100%|██████████| 19/19 [00:03<00:00,  4.83it/s]\n",
            "Accumulating activations: 100%|██████████| 19/19 [00:03<00:00,  4.83it/s]\n"
          ]
        }
      ],
      "source": [
        "# ─── COMPUTE ONE “NUISANCE” VECTOR PER LAYER ───────────────────────────────────────\n",
        "# Use the same accumulate_activations but only for the last token (max_tokens=1)\n",
        "accumulate_activations(positive_nuisance_prompts, nuisance_positive_sums, num_layers, max_tokens=1)\n",
        "accumulate_activations(negative_nuisance_prompts, nuisance_negative_sums, num_layers, max_tokens=1)\n",
        "\n",
        "# Average & normalize per layer\n",
        "pairwise_nuisance = {}\n",
        "for layer_idx in range(num_layers):\n",
        "    mean_pos = nuisance_positive_sums[layer_idx][0] / num_nuisance_pairs\n",
        "    mean_neg = nuisance_negative_sums[layer_idx][0] / num_nuisance_pairs\n",
        "    diff     = mean_pos - mean_neg\n",
        "    pairwise_nuisance[layer_idx] = diff / diff.norm()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHYHySPlUQm1",
        "outputId": "5ca1ed14-46d4-4c95-af2c-3b1dfa0121a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Projected 320 vectors out of 320 mean-diff vectors\n"
          ]
        }
      ],
      "source": [
        "projected_vectors_by_layer = defaultdict(list)\n",
        "\n",
        "for layer_idx, mean_diff_list in layer_mean_diff_vectors.items():\n",
        "    nuisance_vec = pairwise_nuisance[layer_idx]\n",
        "    nuisance_unit = nuisance_vec / nuisance_vec.norm()\n",
        "\n",
        "    for mean_diff in mean_diff_list:\n",
        "        residual = mean_diff.clone()\n",
        "        proj_coef = (residual @ nuisance_unit) / (nuisance_unit.norm() ** 2)\n",
        "        residual = residual - proj_coef * nuisance_unit\n",
        "        residual = residual / residual.norm()\n",
        "        projected_vectors_by_layer[layer_idx].append(residual)\n",
        "\n",
        "total_projected = sum(len(v) for v in projected_vectors_by_layer.values())\n",
        "total_original  = sum(len(v) for v in layer_mean_diff_vectors.values())\n",
        "\n",
        "print(f\"Projected {total_projected} vectors out of {total_original} mean-diff vectors\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F4tzdRyX7R01",
        "outputId": "295ec8bf-95ba-40b0-c9a5-a4a56adddea6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created vector\n"
          ]
        }
      ],
      "source": [
        "with open(\"steering_vector_final\", \"wb\") as f:\n",
        "    pickle.dump(projected_vectors_by_layer, f)\n",
        "\n",
        "print(\"Created vector\")\n",
        "print(\":)\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "038ad0026e8c424e8ca29f384d5b770b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0bc61ab4f952436a939dd88d6d807d67": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1e6789972dfc4a7c86c534c798af2c53",
              "IPY_MODEL_f65593664a2045368b4222a9c7a6179c",
              "IPY_MODEL_90bba208638d49dbb333f25974e90ee8"
            ],
            "layout": "IPY_MODEL_c9f50348c6f94090bed18d2ed47fbd8e"
          }
        },
        "1e6789972dfc4a7c86c534c798af2c53": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7be4cccb457b4a1ab23153dd76f16ee8",
            "placeholder": "​",
            "style": "IPY_MODEL_a2db30416a3a4ccea1c80434922d50ed",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "37d1462b47534c969f730bd3c4c1e884": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7be4cccb457b4a1ab23153dd76f16ee8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90bba208638d49dbb333f25974e90ee8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_038ad0026e8c424e8ca29f384d5b770b",
            "placeholder": "​",
            "style": "IPY_MODEL_37d1462b47534c969f730bd3c4c1e884",
            "value": " 4/4 [00:18&lt;00:00,  3.79s/it]"
          }
        },
        "a2db30416a3a4ccea1c80434922d50ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bc3395916738413b85bb8b2c33f848a2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9f50348c6f94090bed18d2ed47fbd8e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de13cd11430b488bbd1507957d87a0b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f65593664a2045368b4222a9c7a6179c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bc3395916738413b85bb8b2c33f848a2",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_de13cd11430b488bbd1507957d87a0b0",
            "value": 4
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
