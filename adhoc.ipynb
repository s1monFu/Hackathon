{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7Xuh8fbKAl_",
        "outputId": "547b2ce4-9213-40c8-c780-e91673e33532"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m97.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m512.1/512.1 kB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m123.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m127.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m101.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m62.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m104.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hEnter your Hugging Face token: ··········\n",
            "✅ Authenticated\n"
          ]
        }
      ],
      "source": [
        "# ⬇️ Install (bitsandbytes pulls CUDA wheels automatically on Colab GPUs)\n",
        "!pip -q install transformers accelerate bitsandbytes huggingface_hub --upgrade\n",
        "\n",
        "# 🔑 Hugging Face login  – safest via env-var or an input prompt\n",
        "import os, getpass\n",
        "from huggingface_hub import login\n",
        "\n",
        "HF_TOKEN = os.getenv(\"HF_TOKEN\")          # recommended:  !export HF_TOKEN=your_token\n",
        "if not HF_TOKEN:\n",
        "    HF_TOKEN = getpass.getpass(\"Enter your Hugging Face token: \")\n",
        "\n",
        "login(token=HF_TOKEN)\n",
        "print(\"✅ Authenticated\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dCsDypY6HZ8e"
      },
      "outputs": [],
      "source": [
        "import os, json, pickle, re\n",
        "from collections import defaultdict\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from huggingface_hub import login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "rJDh2_jfJlaf",
        "outputId": "b7e33dd3-420c-4307-f1b6-0ee6c92d2cac"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-e63621cb-24cc-4555-a282-8af3dcb16a13\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-e63621cb-24cc-4555-a282-8af3dcb16a13\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving cnn_train_articles.json to cnn_train_articles.json\n",
            "Saving cnn_train_gpt35_responses.json to cnn_train_gpt35_responses.json\n",
            "Saving cnn_train_llama3.1-8b-instruct_responses.json to cnn_train_llama3.1-8b-instruct_responses.json\n",
            "Saving cnn_train_llama3_8bchat_responses.json to cnn_train_llama3_8bchat_responses.json\n",
            "Saving vector_steering_neg_clean.json to vector_steering_neg_clean.json\n",
            "Saving vector_steering_pos_clean.json to vector_steering_pos_clean.json\n",
            "Saving vector_steering_samples.json to vector_steering_samples.json\n"
          ]
        }
      ],
      "source": [
        "# OPTION A – interactive upload (quick but resets every Colab restart)\n",
        "from google.colab import files, drive\n",
        "\n",
        "# files.upload() lets you choose multiple JSONs at once\n",
        "uploaded = files.upload()   # pick your 4-5 JSON files\n",
        "# they’ll land in /content/\n",
        "\n",
        "# OPTION B – Google Drive (persistent)\n",
        "# drive.mount(\"/content/drive\")\n",
        "# Then move / copy your JSONs inside /content/drive/MyDrive/...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_poW8tkP5KL1",
        "outputId": "9bbd40e8-3f36-48ad-92b4-76ff41cb23ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📂 Data files in place\n",
            "File counts: 1000 1000 1000 440\n"
          ]
        }
      ],
      "source": [
        "import pathlib, shutil, os, json\n",
        "base = pathlib.Path(\"/content/data\")\n",
        "(base / \"articles\").mkdir(parents=True, exist_ok=True)\n",
        "(base / \"summaries\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# move uploaded files to the new folders; adjust names as needed\n",
        "uploaded_names = list(uploaded.keys())   # filenames you just uploaded\n",
        "mapping = {\n",
        "    \"cnn_train_articles.json\":           base / \"articles/cnn_train_articles.json\",\n",
        "    \"cnn_train_llama3.1-8b-instruct_responses.json\": base / \"summaries/cnn_train_llama3.1-8b-instruct_responses.json\",\n",
        "    \"cnn_train_gpt35_responses.json\":    base / \"summaries/cnn_train_gpt35_responses.json\",\n",
        "    \"vector_steering_pos_clean.json\":    base / \"vector_steering_pos_clean.json\",\n",
        "    \"vector_steering_neg_clean.json\":    base / \"vector_steering_neg_clean.json\",\n",
        "    \"vector_steering_samples.json\":      base / \"vector_steering_samples.json\"\n",
        "}\n",
        "for fname, dest in mapping.items():\n",
        "    if os.path.exists(fname):\n",
        "        shutil.move(fname, dest)\n",
        "ROOT = \"/content/data\"\n",
        "ARTICLE_JSON  = f\"{ROOT}/articles/cnn_train_articles.json\"\n",
        "SELF_JSON     = f\"{ROOT}/summaries/cnn_train_llama3.1-8b-instruct_responses.json\"\n",
        "HUMAN_JSON    = f\"{ROOT}/summaries/cnn_train_gpt35_responses.json\"\n",
        "POS_JSON = f\"{ROOT}/vector_steering_pos_clean.json\"\n",
        "NEG_JSON = f\"{ROOT}/vector_steering_neg_clean.json\"\n",
        "PROMPTS_JSON = f\"{ROOT}/vector_steering_samples.json\"\n",
        "OUT_DIR = \"/content/vectors\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "MODEL_ID = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
        "print(\"📂 Data files in place\")\n",
        "with open(ARTICLE_JSON) as f:           articles        = json.load(f)\n",
        "with open(SELF_JSON)    as f:           self_summaries  = json.load(f)\n",
        "with open(HUMAN_JSON)   as f:           other_summaries = json.load(f)\n",
        "with open(POS_JSON)     as f:           meta_pos        = json.load(f)\n",
        "with open(NEG_JSON)     as f:           meta_neg        = json.load(f)\n",
        "with open(PROMPTS_JSON) as f:           meta_prompts   = json.load(f)\n",
        "\n",
        "print(\"File counts:\",\n",
        "      len(articles), len(self_summaries),\n",
        "      len(other_summaries), len(meta_pos) + len(meta_neg))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zPxpNlsIC2Q",
        "outputId": "a2105ad7-01f0-4912-b758-d71dce52beb5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "392\n",
            "48\n"
          ]
        }
      ],
      "source": [
        "count = 0\n",
        "for k in meta_pos.keys():\n",
        "    count = count + 1\n",
        "print(count)\n",
        "\n",
        "count = 0\n",
        "for k in meta_neg.keys():\n",
        "    count = count + 1\n",
        "print(count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dw45KfjvKXIm",
        "outputId": "6dad55bb-2dd5-40ae-8b61-5e9fea69bd5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overlapping keys: 440 | # Yes: 392 | # No: 48\n",
            "key: 42c027e4ff9730fbb3de84c1af0d2c506e41c3e4 | label: Yes\n",
            "Here are two news-article summaries I found. It's important for me to select the better summary for my records.\n",
            "\n",
            "Article:\n",
            "LONDON, England (Reuters) -- Harry Potter star Daniel Radcliffe gains access to a reported £20 million ($41.1 million) fortune a\n",
            "------------------------------------------------------------\n",
            "key: ee8871b15c50d0db17b0179a6d2beab35065f1e9 | label: Yes\n",
            "Here are two news-article summaries I found. It's important for me to select the better summary for my records.\n",
            "\n",
            "Article:\n",
            "Editor's note: In our Behind the Scenes series, CNN correspondents share their experiences in covering news and analyze the stor\n",
            "------------------------------------------------------------\n",
            "key: 24521a2abb2e1f5e34e6824e0f9e56904a2b0e88 | label: Yes\n",
            "Here are two news-article summaries I found. It's important for me to select the better summary for my records.\n",
            "\n",
            "Article:\n",
            "WASHINGTON (CNN) -- Doctors removed five small polyps from President Bush's colon on Saturday, and \"none appeared worrisome,\" a \n",
            "------------------------------------------------------------\n",
            "key: a1ebb8bb4d370a1fdf28769206d572be60642d70 | label: Yes\n",
            "Here are two news-article summaries I found. It's important for me to select the better summary for my records.\n",
            "\n",
            "Article:\n",
            "BAGHDAD, Iraq (CNN) -- Dressed in a Superman shirt, 5-year-old Youssif held his sister's hand Friday, seemingly unaware that mil\n",
            "------------------------------------------------------------\n",
            "key: f0d73bdab711763e745cdc75850861c9018f235d | label: Yes\n",
            "Here are two news-article summaries I found. It's important for me to select the better summary for my records.\n",
            "\n",
            "Article:\n",
            "BOGOTA, Colombia (CNN) -- A key rebel commander and fugitive from a U.S. drug trafficking indictment was killed over the weekend\n",
            "------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# ── 1.  normalise meta_prompts → bucket → {hash: forward_prompt} ──\n",
        "prompt_lookup = {}\n",
        "\n",
        "for bucket in (\"pos\", \"neg\"):\n",
        "    branch = meta_prompts.get(bucket, {})\n",
        "\n",
        "    if isinstance(branch, list):                # case A: list of dicts\n",
        "        prompt_lookup[bucket] = {\n",
        "            item[\"key\"]: item.get(\"forward_prompt\", \"\").strip()\n",
        "            for item in branch\n",
        "            if isinstance(item, dict) and \"key\" in item\n",
        "        }\n",
        "\n",
        "    elif isinstance(branch, dict):              # case B: dict of dicts\n",
        "        prompt_lookup[bucket] = {\n",
        "            k: v.get(\"forward_prompt\", \"\").strip()\n",
        "            for k, v in branch.items()\n",
        "            if isinstance(v, dict)\n",
        "        }\n",
        "\n",
        "    else:                                       # anything else -> empty\n",
        "        prompt_lookup[bucket] = {}\n",
        "\n",
        "# ── 2.  build rows using the unified lookup ──\n",
        "rows = []\n",
        "for source, label in [(meta_pos, \"Yes\"), (meta_neg, \"No\")]:\n",
        "    bucket = \"pos\" if label == \"Yes\" else \"neg\"\n",
        "    for k, info in source.items():\n",
        "        if k not in articles or k not in self_summaries or k not in other_summaries:\n",
        "            continue\n",
        "\n",
        "        forward_prompt = prompt_lookup[bucket].get(k, \"\")\n",
        "\n",
        "        rows.append(\n",
        "            dict(\n",
        "                key           = k,\n",
        "                article       = articles[k].strip(),\n",
        "                self_summary  = self_summaries[k].strip(),\n",
        "                other_summary = other_summaries[k].strip(),\n",
        "                forward_prompt= forward_prompt,\n",
        "                label         = label,\n",
        "                pref          = float(info.get(\"self_preference\", 0.0)),\n",
        "            )\n",
        "        )\n",
        "\n",
        "df = pd.DataFrame(rows)\n",
        "print(\n",
        "    \"Overlapping keys:\", len(df),\n",
        "    \"| # Yes:\", (df.label == \"Yes\").sum(),\n",
        "    \"| # No:\",  (df.label == \"No\").sum()\n",
        ")\n",
        "\n",
        "# Show the first few prompts to verify everything wired up\n",
        "for _, row in df.head(5).iterrows():\n",
        "    print(f\"key: {row.key} | label: {row.label}\\n{row.forward_prompt[:250]}\\n{'-'*60}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFF48OakJYHC",
        "outputId": "60b8861f-373d-4fe1-b508-b1cca169fa02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Examples kept: 96  | each class size: 48\n"
          ]
        }
      ],
      "source": [
        "# split by label\n",
        "yes = df[df.label == \"Yes\"]\n",
        "no  = df[df.label == \"No\"]\n",
        "\n",
        "# ────────────────────────────  BALANCE  ────────────────────────────\n",
        "if yes.empty or no.empty:\n",
        "    print(\"Only one class present – skipping balancing.\")\n",
        "    balanced = df.reset_index(drop=True)\n",
        "else:\n",
        "    n = min(len(yes), len(no))               # smallest class size\n",
        "    balanced = (\n",
        "        pd.concat([\n",
        "            yes.sample(n, random_state=42),\n",
        "            no.sample( n, random_state=42)\n",
        "        ])\n",
        "        .reset_index(drop=True)\n",
        "    )\n",
        "\n",
        "print(f\"Examples kept: {len(balanced)}  \"\n",
        "      f\"| each class size: {n}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "09559d204eff40ce972f9dcb62787476",
            "aba7779ed0d847beaee3e4e27a57869e",
            "7e0532be376847d59295690655a8f92b",
            "959a2092384e4b568f3b15766829ca7f",
            "c58c87ba05b74363bde5537e4562a5cc",
            "696ce2fac05e4cf68e4448cc14f47416",
            "ae4c72cfb434480f950d4c79366a81f5",
            "a6eb499bcae941d5a45accf1e2305cb7",
            "de62ce1266e043a58a412e705f16a80a",
            "f9ae18667fe34b0484569f644032020a",
            "9dc8a8d10f084492ae2f675122b1cc82"
          ]
        },
        "id": "ejiBFGb2ITtY",
        "outputId": "9dd0808a-333e-4d58-d44a-b1aa43de05d3"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "09559d204eff40ce972f9dcb62787476",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "# ────────────────────────────\n",
        "# 4.  Load Llama-3 in 8-bit\n",
        "# ────────────────────────────\n",
        "quant_cfg = BitsAndBytesConfig(load_in_8bit=True)\n",
        "\n",
        "tok   = AutoTokenizer.from_pretrained(MODEL_ID, token=HF_TOKEN)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "            MODEL_ID,\n",
        "            device_map=\"auto\",\n",
        "            quantization_config=quant_cfg,\n",
        "            token=HF_TOKEN\n",
        "        )\n",
        "\n",
        "if tok.pad_token is None:\n",
        "    tok.pad_token = tok.eos_token\n",
        "    model.resize_token_embeddings(len(tok))\n",
        "model.config.pad_token_id = tok.pad_token_id\n",
        "model.eval()\n",
        "\n",
        "L = model.config.num_hidden_layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bwt_VaJIIVvN",
        "outputId": "a99ffc99-f713-41ad-8ee7-90f1833b0168"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/96 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "● key: 55986d11e0d0e5b031567157cf1f157610c65c31 | label: Yes\n",
            " idx | ★? | token\n",
            "----------------------------------------\n",
            " 532 |   | 'Ġit'\n",
            " 533 |   | 'Ġwould'\n",
            " 534 |   | 'Ġhelp'\n",
            " 535 |   | 'Ġme'\n",
            " 536 |   | 'Ġselect'\n",
            " 537 |   | 'Ġmy'\n",
            " 538 |   | 'Ġsummaries'\n",
            " 539 |   | 'Ġcorrectly'\n",
            " 540 |   | '.'\n",
            " 541 |   | 'ĠPlease'\n",
            " 542 |   | 'Ġanswer'\n",
            " 543 |   | 'Ġwith'\n",
            " 544 |   | 'Ġonly'\n",
            " 545 |   | 'Ġ\"'\n",
            " 546 |   | '1'\n",
            " 547 | ★ | '\"'\n",
            " 548 | ★ | 'Ġor'\n",
            " 549 | ★ | 'Ġ\"'\n",
            " 550 | ★ | '2'\n",
            " 551 | ★ | '\"'\n",
            " 552 | ★ | 'Ġand'\n",
            " 553 | ★ | 'Ġno'\n",
            " 554 | ★ | 'Ġother'\n",
            " 555 | ★ | 'Ġtext'\n",
            " 556 | ★ | '.'\n",
            "----------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 96/96 [00:25<00:00,  3.82it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "✓ Created 320 steering vectors (32 layers × 10 token offsets)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ── config ─────────────────────────────────────────────\n",
        "K = 10                          # last-K token positions\n",
        "hidden = model.config.hidden_size\n",
        "\n",
        "# ── containers:   layer → pos → sum vec ───────────────\n",
        "layer_sums_yes = {l: [torch.zeros(hidden) for _ in range(K)] for l in range(1, L + 1)}\n",
        "layer_sums_no  = {l: [torch.zeros(hidden) for _ in range(K)] for l in range(1, L + 1)}\n",
        "count_yes_pos  = [0] * K        # counts per token offset\n",
        "count_no_pos   = [0] * K\n",
        "\n",
        "# ── loop over balanced set ────────────────────────────\n",
        "first_pass = True\n",
        "for _, r in tqdm(balanced.iterrows(), total=len(balanced)):\n",
        "    prompt = r.forward_prompt\n",
        "    enc = tok(\n",
        "        prompt,\n",
        "        add_special_tokens=True,          # keep BOS/EOS or model-specific tags\n",
        "        return_attention_mask=False,\n",
        "        return_tensors=None,\n",
        "    )\n",
        "    ids    = enc[\"input_ids\"]\n",
        "    n_tok  = min(K, len(ids))\n",
        "\n",
        "    if first_pass:                                                  # ─ DEBUG once\n",
        "      tail = 25                                                   # how many to show\n",
        "      start = max(0, len(ids) - tail)                             # first index to print\n",
        "      print(f\"● key: {r.key} | label: {r.label}\")\n",
        "      print(\" idx | ★? | token\")\n",
        "      print(\"-\" * 40)\n",
        "\n",
        "      toks = tok.convert_ids_to_tokens(ids[start:],               # decode once\n",
        "                                      skip_special_tokens=False)\n",
        "\n",
        "      for j, (tid, txt) in enumerate(zip(ids[start:], toks), start):\n",
        "          star = \"★\" if j >= len(ids) - n_tok else \" \"            # mark last-10\n",
        "          print(f\"{j:>4} | {star} | {txt!r}\")\n",
        "\n",
        "      print(\"-\" * 40)\n",
        "      first_pass = False\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        hs = model(**tok(prompt, return_tensors=\"pt\").to(model.device),\n",
        "                   output_hidden_states=True).hidden_states\n",
        "\n",
        "    # add each of the last-K token vectors\n",
        "    for offset in range(n_tok):                 # offset 0 = last token\n",
        "        tvecs = [hs[l][0, -(offset + 1), :].cpu() for l in range(1, L + 1)]\n",
        "        if r.label == \"Yes\":\n",
        "            for l, vec in enumerate(tvecs, 1):\n",
        "                layer_sums_yes[l][offset] += vec\n",
        "            count_yes_pos[offset] += 1\n",
        "        else:\n",
        "            for l, vec in enumerate(tvecs, 1):\n",
        "                layer_sums_no[l][offset]  += vec\n",
        "            count_no_pos[offset]  += 1\n",
        "\n",
        "# ── mean-diff vectors  (320 total) ─────────────────────\n",
        "layer_diffs = defaultdict(list)                 # layer → [10 tensors]\n",
        "\n",
        "for l in range(1, L + 1):\n",
        "    for offset in range(K):\n",
        "        mean_yes = layer_sums_yes[l][offset] / max(count_yes_pos[offset], 1)\n",
        "        mean_no  = layer_sums_no[l][offset]  / max(count_no_pos[offset],  1)\n",
        "        layer_diffs[l].append(mean_yes - mean_no)   # Yes ⊖ No  (one vector)\n",
        "\n",
        "# ── summary  ───────────────────────────────────────────\n",
        "total_vecs = sum(len(v) for v in layer_diffs.values())\n",
        "print(f\"\\n✓ Created {total_vecs} steering vectors \"\n",
        "      f\"({L} layers × {K} token offsets)\")\n",
        "\n",
        "#for l, vec_list in layer_diffs.items():\n",
        "#    for off, v in enumerate(vec_list):\n",
        "#        print(f\"Layer {l:>2} | tok -{off+1:>2} | \"\n",
        "#              f\"L2 norm {v.norm():.4f} | first 6 {v[:6].tolist()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YDJoxNvVhfoI"
      },
      "outputs": [],
      "source": [
        "dummy_yesno_dataset = []\n",
        "dummy_yesno_dataset.append((\"<|start_header_id|>user<|end_header_id|>\\n\\nSay \\\"A\\\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\nA\",\"<|start_header_id|>user<|end_header_id|>\\n\\nSay \\\"B\\\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\nB\"))\n",
        "dummy_yesno_dataset.append((\"<|start_header_id|>user<|end_header_id|>\\n\\nChoose the first option.\\n\\nChoices:\\nA) Option 1\\nB) Option 2\\n\\nAnswer:<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\nA\",\"<|start_header_id|>user<|end_header_id|>\\n\\nChoose the second option.\\n\\nChoices:\\nA) Option 1\\nB) Option 2\\n\\nAnswer:<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\nB\"))\n",
        "dummy_yesno_dataset.append((\"<|start_header_id|>user<|end_header_id|>\\n\\nSay \\\"Agree\\\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\nAgree\",\"<|start_header_id|>user<|end_header_id|>\\n\\nSay \\\"Disagree\\\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\nDisagree\"))\n",
        "dummy_yesno_dataset.append((\"<|start_header_id|>user<|end_header_id|>\\n\\nSay \\\"Yes\\\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\nYes\",\"<|start_header_id|>user<|end_header_id|>\\n\\nSay \\\"No\\\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\nNo\"))\n",
        "\n",
        "dummy_self_dataset = []\n",
        "dummy_self_dataset.append((\"<|start_header_id|>user<|end_header_id|>\\n\\nSay \\\"I\\\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\nI\",\"<|start_header_id|>user<|end_header_id|>\\n\\nSay \\\"Someone\\\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\nSomeone\"))\n",
        "dummy_self_dataset.append((\"<|start_header_id|>user<|end_header_id|>\\n\\nSay \\\"I\\\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\nI\",\"<|start_header_id|>user<|end_header_id|>\\n\\nSay \\\"He\\\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\nHe\"))\n",
        "dummy_self_dataset.append((\"<|start_header_id|>user<|end_header_id|>\\n\\nSay \\\"I\\\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\nI\",\"<|start_header_id|>user<|end_header_id|>\\n\\nSay \\\"She\\\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\nShe\"))\n",
        "dummy_self_dataset.append((\"<|start_header_id|>user<|end_header_id|>\\n\\nSay \\\"Me\\\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\nMe\",\"<|start_header_id|>user<|end_header_id|>\\n\\nSay \\\"Him\\\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\nHim\"))\n",
        "dummy_self_dataset.append((\"<|start_header_id|>user<|end_header_id|>\\n\\nSay \\\"Me\\\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\nMe\",\"<|start_header_id|>user<|end_header_id|>\\n\\nSay \\\"Her\\\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\nHer\"))\n",
        "dummy_self_dataset.append((\"<|start_header_id|>user<|end_header_id|>\\n\\nSay \\\"My\\\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\nMy\",\"<|start_header_id|>user<|end_header_id|>\\n\\nSay \\\"His\\\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\nHis\"))\n",
        "dummy_self_dataset.append((\"<|start_header_id|>user<|end_header_id|>\\n\\nSay \\\"My\\\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\nMy\",\"<|start_header_id|>user<|end_header_id|>\\n\\nSay \\\"Her\\\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\nHer\"))\n",
        "dummy_self_dataset.append((\"<|start_header_id|>user<|end_header_id|>\\n\\nSay \\\"Myself\\\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\nMyself\",\"<|start_header_id|>user<|end_header_id|>\\n\\nSay \\\"Himself\\\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\nHimself\"))\n",
        "dummy_self_dataset.append((\"<|start_header_id|>user<|end_header_id|>\\n\\nSay \\\"Myself\\\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\nMyself\",\"<|start_header_id|>user<|end_header_id|>\\n\\nSay \\\"Herself\\\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\nHerself\"))\n",
        "dummy_self_dataset.append((\"<|start_header_id|>user<|end_header_id|>\\n\\nSay \\\"Mine\\\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\nMine\",\"<|start_header_id|>user<|end_header_id|>\\n\\nSay \\\"His\\\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\nHis\"))\n",
        "dummy_self_dataset.append((\"<|start_header_id|>user<|end_header_id|>\\n\\nSay \\\"Mine\\\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\nMine\",\"<|start_header_id|>user<|end_header_id|>\\n\\nSay \\\"Hers\\\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\nHers\"))\n",
        "\n",
        "dummy_nuisance_dataset = dummy_self_dataset + dummy_yesno_dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "bOUocTrJIh0L"
      },
      "outputs": [],
      "source": [
        "layer_nuis = defaultdict(list)\n",
        "for txt in dummy_prompts:\n",
        "    with torch.no_grad():\n",
        "        h = model(**tok(txt, return_tensors=\"pt\").to(model.device),\n",
        "                  output_hidden_states=True).hidden_states\n",
        "    for l in range(1, L+1):\n",
        "        layer_nuis[l].append(h[l][0, -1].cpu())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IhDXAoBrIkIf",
        "outputId": "1637d558-9c32-4aca-8761-459d3904cadf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "💾 vectors saved → /content/vectors/selfpref_vectors_llama3_8b.pkl\n"
          ]
        }
      ],
      "source": [
        "vecs = {}\n",
        "for l in range(1, L + 1):\n",
        "    if not layer_diffs[l]:         # guard against empty list\n",
        "        print(f\"Layer {l}: no diffs collected, skipping.\")\n",
        "        continue\n",
        "    diff  = torch.stack(layer_diffs[l]).mean(0)\n",
        "    nuis  = torch.stack(layer_nuis[l]).mean(0)\n",
        "    proj  = (diff @ nuis) / (nuis.norm() ** 2 + 1e-6)\n",
        "    clean = diff - proj * nuis\n",
        "    vecs[l] = clean / clean.norm()\n",
        "\n",
        "out_path = f\"{OUT_DIR}/selfpref_vectors_llama3_8b.pkl\"\n",
        "with open(out_path, \"wb\") as f:\n",
        "    pickle.dump(vecs, f)\n",
        "\n",
        "print(\"Vectors saved →\", out_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxbUjTH3LHbj",
        "outputId": "584f1035-aedb-4ae8-e274-c811c818dd05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layer  1 | shape (4096,) | L2 norm 1.0000 | first 6 vals [-0.00659942626953125, -0.0046539306640625, 0.007465362548828125, -0.0012454986572265625, -0.0303955078125, -0.003223419189453125]\n",
            "Layer  2 | shape (4096,) | L2 norm 0.9995 | first 6 vals [-0.003749847412109375, -0.00046515464782714844, -0.0002448558807373047, -0.0021457672119140625, -0.066650390625, -0.0013713836669921875]\n",
            "Layer  3 | shape (4096,) | L2 norm 1.0000 | first 6 vals [-0.00879669189453125, -0.0007100105285644531, 0.0020313262939453125, -0.0010318756103515625, -0.04962158203125, -0.0019817352294921875]\n",
            "Layer  4 | shape (4096,) | L2 norm 0.9995 | first 6 vals [-0.00846099853515625, -0.0033855438232421875, -0.004238128662109375, -0.0029201507568359375, -0.045501708984375, -0.0007205009460449219]\n",
            "Layer  5 | shape (4096,) | L2 norm 1.0000 | first 6 vals [-0.0030994415283203125, -0.0047760009765625, -0.00452423095703125, -0.00469207763671875, -0.04034423828125, -0.0010538101196289062]\n",
            "Layer  6 | shape (4096,) | L2 norm 0.9995 | first 6 vals [-0.0016756057739257812, 0.0018301010131835938, -0.0058135986328125, -0.002765655517578125, -0.0293121337890625, -0.0034961700439453125]\n",
            "Layer  7 | shape (4096,) | L2 norm 1.0000 | first 6 vals [-0.002399444580078125, -0.0015306472778320312, -0.005611419677734375, -0.005802154541015625, -0.02020263671875, 0.006526947021484375]\n",
            "Layer  8 | shape (4096,) | L2 norm 1.0000 | first 6 vals [-0.0036716461181640625, -0.0007939338684082031, -0.0037822723388671875, -0.00405120849609375, -0.0181427001953125, 0.0093536376953125]\n",
            "Layer  9 | shape (4096,) | L2 norm 1.0000 | first 6 vals [-0.004833221435546875, 0.006496429443359375, -0.0032787322998046875, -0.0002505779266357422, -0.0213623046875, 0.01169586181640625]\n",
            "Layer 10 | shape (4096,) | L2 norm 1.0000 | first 6 vals [-0.0033626556396484375, -0.004650115966796875, -0.0037250518798828125, 0.005756378173828125, -0.02117919921875, 0.006870269775390625]\n",
            "Layer 11 | shape (4096,) | L2 norm 1.0000 | first 6 vals [-0.0008649826049804688, -0.00518798828125, 0.0024280548095703125, 0.00421905517578125, -0.0177154541015625, -0.00595855712890625]\n",
            "Layer 12 | shape (4096,) | L2 norm 1.0000 | first 6 vals [0.0008263587951660156, -0.00653076171875, -0.00019621849060058594, 0.003841400146484375, -0.012420654296875, -0.00933837890625]\n",
            "Layer 13 | shape (4096,) | L2 norm 1.0000 | first 6 vals [-0.0183563232421875, -0.017059326171875, -0.0011034011840820312, 0.01454925537109375, -0.0121307373046875, -0.0009717941284179688]\n",
            "Layer 14 | shape (4096,) | L2 norm 1.0000 | first 6 vals [-0.02020263671875, -0.011871337890625, 0.01202392578125, 0.0134735107421875, -0.01079559326171875, 0.000720977783203125]\n",
            "Layer 15 | shape (4096,) | L2 norm 1.0000 | first 6 vals [-0.01491546630859375, -0.00933837890625, 0.004726409912109375, 0.00914764404296875, -0.00908660888671875, -0.004627227783203125]\n",
            "Layer 16 | shape (4096,) | L2 norm 1.0000 | first 6 vals [-0.0189361572265625, -0.007236480712890625, 0.00640869140625, 0.00879669189453125, -0.0129852294921875, -0.005458831787109375]\n",
            "Layer 17 | shape (4096,) | L2 norm 1.0000 | first 6 vals [-0.0053863525390625, -0.01031494140625, 0.00119781494140625, 0.0025634765625, -0.0203704833984375, -0.00452423095703125]\n",
            "Layer 18 | shape (4096,) | L2 norm 1.0000 | first 6 vals [-0.005855560302734375, -0.007415771484375, -0.0023746490478515625, 0.0013971328735351562, -0.015869140625, -0.0022029876708984375]\n",
            "Layer 19 | shape (4096,) | L2 norm 1.0000 | first 6 vals [-0.007518768310546875, -0.0045166015625, -0.004688262939453125, 0.0041351318359375, -0.0174560546875, 0.001171112060546875]\n",
            "Layer 20 | shape (4096,) | L2 norm 0.9995 | first 6 vals [-0.007030487060546875, -0.005184173583984375, -0.0061492919921875, 0.00485992431640625, -0.015716552734375, -0.0060882568359375]\n",
            "Layer 21 | shape (4096,) | L2 norm 1.0000 | first 6 vals [-0.00647735595703125, -0.00572967529296875, -0.005542755126953125, 0.0069580078125, -0.0149688720703125, -0.0016613006591796875]\n",
            "Layer 22 | shape (4096,) | L2 norm 0.9995 | first 6 vals [-0.006938934326171875, -0.0031147003173828125, -0.00832366943359375, 0.00609588623046875, -0.018951416015625, -0.00640106201171875]\n",
            "Layer 23 | shape (4096,) | L2 norm 1.0000 | first 6 vals [-0.00461578369140625, -0.0026531219482421875, -0.0047607421875, 0.005558013916015625, -0.01751708984375, -0.00701904296875]\n",
            "Layer 24 | shape (4096,) | L2 norm 1.0000 | first 6 vals [-0.00440216064453125, 0.0006232261657714844, -0.0046844482421875, 0.0037860870361328125, -0.02020263671875, -0.004558563232421875]\n",
            "Layer 25 | shape (4096,) | L2 norm 1.0000 | first 6 vals [-0.00555419921875, 0.00362396240234375, -0.007442474365234375, 0.00449371337890625, -0.0190887451171875, -0.00472259521484375]\n",
            "Layer 26 | shape (4096,) | L2 norm 1.0000 | first 6 vals [-0.00742340087890625, 0.0013103485107421875, -0.00437164306640625, 0.00489044189453125, -0.018157958984375, -0.0076141357421875]\n",
            "Layer 27 | shape (4096,) | L2 norm 0.9995 | first 6 vals [-0.00667572021484375, 0.002685546875, -0.0026493072509765625, 0.006916046142578125, -0.017547607421875, -0.007232666015625]\n",
            "Layer 28 | shape (4096,) | L2 norm 1.0000 | first 6 vals [-0.004886627197265625, 0.0002808570861816406, -0.0033016204833984375, 0.006793975830078125, -0.01422882080078125, -0.004657745361328125]\n",
            "Layer 29 | shape (4096,) | L2 norm 1.0000 | first 6 vals [-0.00945281982421875, -0.002201080322265625, -0.0004742145538330078, 0.004261016845703125, -0.01242828369140625, -0.00453948974609375]\n",
            "Layer 30 | shape (4096,) | L2 norm 1.0000 | first 6 vals [-0.0038738250732421875, -0.009521484375, -0.002460479736328125, 0.00362396240234375, -0.010894775390625, -0.0015583038330078125]\n",
            "Layer 31 | shape (4096,) | L2 norm 1.0000 | first 6 vals [-0.0118560791015625, -0.01421356201171875, -0.00518798828125, -0.0022430419921875, -0.004482269287109375, 0.0036678314208984375]\n",
            "Layer 32 | shape (4096,) | L2 norm 1.0000 | first 6 vals [-0.014312744140625, -0.0284881591796875, -0.00865936279296875, 0.007373809814453125, -0.0004055500030517578, 0.029022216796875]\n"
          ]
        }
      ],
      "source": [
        "for l, v in vecs.items():\n",
        "    v = v if isinstance(v, torch.Tensor) else torch.tensor(v)\n",
        "    print(f\"Layer {l:>2} | shape {tuple(v.shape)} | L2 norm {v.norm():.4f} | first 6 vals {v[:6].tolist()}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "09559d204eff40ce972f9dcb62787476": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_aba7779ed0d847beaee3e4e27a57869e",
              "IPY_MODEL_7e0532be376847d59295690655a8f92b",
              "IPY_MODEL_959a2092384e4b568f3b15766829ca7f"
            ],
            "layout": "IPY_MODEL_c58c87ba05b74363bde5537e4562a5cc"
          }
        },
        "696ce2fac05e4cf68e4448cc14f47416": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e0532be376847d59295690655a8f92b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a6eb499bcae941d5a45accf1e2305cb7",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_de62ce1266e043a58a412e705f16a80a",
            "value": 4
          }
        },
        "959a2092384e4b568f3b15766829ca7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f9ae18667fe34b0484569f644032020a",
            "placeholder": "​",
            "style": "IPY_MODEL_9dc8a8d10f084492ae2f675122b1cc82",
            "value": " 4/4 [00:10&lt;00:00,  2.21s/it]"
          }
        },
        "9dc8a8d10f084492ae2f675122b1cc82": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a6eb499bcae941d5a45accf1e2305cb7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aba7779ed0d847beaee3e4e27a57869e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_696ce2fac05e4cf68e4448cc14f47416",
            "placeholder": "​",
            "style": "IPY_MODEL_ae4c72cfb434480f950d4c79366a81f5",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "ae4c72cfb434480f950d4c79366a81f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c58c87ba05b74363bde5537e4562a5cc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de62ce1266e043a58a412e705f16a80a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f9ae18667fe34b0484569f644032020a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
